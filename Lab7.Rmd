---
title: "Lab 7"
font-family: 'Corbel'
author: Marjan Rezvani, Patrick Sinclair, Kieran Yuen
output: github_document
---

```{r Load NHIS, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
load("NHIS_2014.RData")
```

```{r Recode Earn_LastYr}
data_use1$earn_lastyr <- as.factor(data_use1$ERNYR_P)
levels(data_use1$earn_lastyr) <- c("0","$01-$4999","$5000-$9999","$10000-$14999","$15000-$19999","$20000-$24999","$25000-$34999","$35000-$44999","$45000-$54999","$55000-$64999","$65000-$74999","$75000 and over",NA,NA,NA)
```
### Predicting Health Insurance Coverage
Using the provided data from the National Health Interview Survey 2014, we took a subset of respondents between the ages of 17 and 75. We chose this particular range with a view to further possible subgroups, such as examining the changes in insurance coverage from those of college age up to 26 and those in their late 20s and early 30s, who are [no longer eligible](https://www.healthcare.gov/young-adults/children-under-26/) to be claimed on their parents' plans.

Below we have subtotals and proportions for the respondents in the data set between the ages of 17 and 75 who have and do not have health insurance coverage. 
```{r Create Dat2 Props}
dat2 <- subset(data_use1, ((AGE_P >= 17) & (AGE_P <= 75)))
covprop <- dat2$NOTCOV 
covtable <- table(covprop)
agecovtable <- table(dat2$AGE_P, dat2$NOTCOV)
colnames(agecovtable) <- c("Covered", "Not Covered")
agetable <- addmargins(agecovtable)
names(covtable) <- c("Covered", "Not Covered")
t1 <- addmargins(covtable)
t2 <- prop.table(covtable)
kables(list(kable(t1, align = "l", col.names = c("Coverage", "Total")), kable(t2, col.names = c("Coverage", "Proportion"))))
```

Approximately `r round((prop.table(covtable)[1])*100, digits = 0)`% of respondents have health insurance coverage. 

Here are the logit regression results from the amended subset - Age $\ge 17$, $\le 75$.
```{r 1st Logit}
model_logit1 <- glm(NOTCOV ~ AGE_P + I(AGE_P^2) + female + AfAm + Asian + RaceOther  
                    + Hispanic + educ_hs + educ_smcoll + educ_as + educ_bach + educ_adv 
                    + married + widowed + divorc_sep + veteran_stat + REGION + region_born,
                    family = binomial, data = dat2)
summary(model_logit1)
```

Given that the dependent variable NOTCOV is coded 0 and 1. [(p. 202, Person Variable Layout Document) N.B. This link immediately initiates a PDF download](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NHIS/2014/personsx_layout.pdf). The value 1 meaning that the condition of no coverage *is* present, we can interpret from the logit regression that coefficients with a positive value are predictors of **NOT** having health insurance.

As we implement the other machine learning prediction models and interpret their results, we can keep the results of this logit regression in mind, as a guide for what the models are putting emphasis on and how they are treating the variables differently from model to model. For example, when we look at the results from the Support vector Machine, we know that in an SVM, the loss function for observations that fall on the correct side of the margins of the separating plane have no bearing on the position or slope of the separating plane. In a logit regression, the loss function is never exactly equal to zero, so observations that fall well within in each classification zone will still impact the position and slope of the separating plane as those observations change, even if the impact is very small. If the dependent classes are well separated, the logit regression is more susceptible to the impact of outlying determining variables than Support Vector Machines [(James, Witten, Hastie and Tibshirani, p. 357)](https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf).

```{r Factor Recode for data.frame}
d_region <- data.frame(model.matrix(~ dat2$REGION))
d_region_born <- data.frame(model.matrix(~ factor(dat2$region_born)))
dat_for_analysis_sub <- data.frame(
  dat2$NOTCOV,
  dat2$AGE_P,
  dat2$female,
  dat2$AfAm,
  dat2$Asian,
  dat2$RaceOther,
  dat2$Hispanic,
  dat2$educ_hs,
  dat2$educ_smcoll,
  dat2$educ_as,
  dat2$educ_bach,
  dat2$educ_adv,
  dat2$married,
  dat2$widowed,
  dat2$divorc_sep,
  d_region[,2:4],
  d_region_born[,2:12]) # need [] since model.matrix includes intercept term
names(dat_for_analysis_sub) <- c("NOTCOV",
                                 "Age",
                                 "female",
                                 "AfAm",
                                 "Asian",
                                 "RaceOther",
                                 "Hispanic",
                                 "educ_hs",
                                 "educ_smcoll",
                                 "educ_as",
                                 "educ_bach",
                                 "educ_adv",
                                 "married",
                                 "widowed",
                                 "divorc_sep",
                                 "Region.Midwest",
                                 "Region.South",
                                 "Region.West",
                                 "born.Mex.CentAm.Carib",
                                 "born.S.Am",
                                 "born.Eur",
                                 "born.f.USSR",
                                 "born.Africa",
                                 "born.MidE",
                                 "born.India.subc",
                                 "born.Asia",
                                 "born.SE.Asia",
                                 "born.elsewhere",
                                 "born.unknown")
```
We've utilized the provided data.frame and standarizing code to standardize the variables and create our training and test sets. The training set is comprised of 20% of the observations. We noticed that creating factors of REGION and region_born excluded Northeast and [Born in the US](https://www.youtube.com/watch?v=EPhWR4d3FJQ), which will influence the results the models produce. This may be an important factor for the researcher querying the dataset to consider.
```{r Standardize for Sobj, include=FALSE}
require("standardize")
set.seed(654321)
NN <- length(dat_for_analysis_sub$NOTCOV)
NN
restrict_1 <- as.logical(runif(NN) < 0.20) # use fraction as training data
# summary(restrict_1)
dat_train <- subset(dat_for_analysis_sub, restrict_1)
dat_test <- subset(dat_for_analysis_sub, !restrict_1)
sobj <- standardize(NOTCOV ~ Age + female + AfAm + Asian + RaceOther + Hispanic + 
                      educ_hs + educ_smcoll + educ_as + educ_bach + educ_adv + 
                      married + widowed + divorc_sep + 
                      Region.Midwest + Region.South + Region.West + 
                      born.Mex.CentAm.Carib + born.S.Am + born.Eur + born.f.USSR + 
                      born.Africa + born.MidE + born.India.subc + born.Asia + 
                      born.SE.Asia + born.elsewhere + born.unknown, dat_train, family = binomial)
s_dat_test <- predict(sobj, dat_test)
# summary(sobj$data)
```
#### OLS
```{r Linear Prob Model}
model_OLS1 <- lm(sobj$formula, data = sobj$data)
summary(model_OLS1)
pred_vals_OLS <- suppressWarnings(predict(model_OLS1, s_dat_test))
pred_model_OLS1 <- (pred_vals_OLS > 0.45)
pred1OLStable <- table(pred = pred_model_OLS1, true = dat_test$NOTCOV)
pred1OLStable
goodolspred <- sum((prop.table(pred1OLStable)[1,1])+(prop.table(pred1OLStable)[2,2]))
```
#### Logit
```{r Logit}
model_logit1 <- glm(sobj$formula, family = binomial, data = sobj$data)
summary(model_logit1)
pred_vals <- suppressWarnings(predict(model_logit1, s_dat_test, type = "response"))
pred_model_logit1 <- (pred_vals > 0.5)
pred1Logtable <- table(pred = pred_model_logit1, true = dat_test$NOTCOV)
pred1Logtable
goodlogpred <- sum((prop.table(pred1Logtable)[1,1])+(prop.table(pred1Logtable)[2,2]))
# round(goodolspred*100, digits = 2)
```
Given the standardized variables, we can compare the OLS and Logit regressions. While the values of each coefficient vary, the sign for each coefficient is the same across both regressions. The OLS regression is demonstrating the correlation of each variable with the condition of not having health insurance. The logit regression is giving us the probability of the condition of not having health insurance, given each variable.

Due to the standardization of the variables, the coefficients estimated by the OLS regression are all small - considering them as raw values gives us a skewed interpretation of the result; we need to consider where these the absolute values of the coefficients sit on the interval [0,1]. 

The logit regression is more straightforward, as we would expect the probabilities to be given on a [0,1] interval.

Changing the predvals parameter to > 0.45 increases the accuracy of both models, by 0.3% and 0.11% for the OLS and Logit models respectively. The increases are negligible. Leaving the Logit predvals value at > 0.5 makes more sense as 0.5 is a probability value in this model. Setting it to a value below 0.5 would skew the correct prediction rate of whether the dependent variable is equal to 1. The prediction rate of the OLS and Logit models are `r round(goodolspred*100, digits = 2)`% and `r round(goodlogpred*100, digits = 2)`% respectively.

### Random Forest Model
```{r Random Forest}
require('randomForest')
set.seed(54321)
model_randFor <- randomForest(as.factor(NOTCOV) ~ ., data = sobj$data, importance=TRUE, proximity=TRUE)
print(model_randFor)
round(importance(model_randFor),2)
varImpPlot(model_randFor)
# look at confusion matrix for this too
```
![RF Plot](./RFPlot.png)
```{r RFPred}
pred_model1 <- predict(model_randFor,  s_dat_test)
RFpredtable <- table(pred = pred_model1, true = dat_test$NOTCOV)
RFpredtable
```
```{r Proportions of Correct Prediction}
RFproppred <- prop.table(RFpredtable)
# proppred
RFgoodpred <- sum((RFproppred[1,1])+(RFproppred[2,2]))
# goodpred
RFfalsepos <- RFproppred[2,1]
RFfalseneg <- RFproppred[1,2]
# falsepos
# falseneg
# round(goodpred*100, digits = 2)
```
Our Random Forest model gives us a correct prediction rate within the test data of `r round(RFgoodpred*100, digits = 2)`% and it provides us some clearer insight into the variables that are important to the accuracy of the model. We can see from both the Mean Accuracy Decrease and the Mean Gini Decrease that age, the education variables and the race variables are important to the accuracy of the model. Interestingly enough, region of birth is unimportant to the accuracy of this model - the two birth regions, Sth America and Mexico, Central America and the Caribbean with the highest importance link to the race variable with the highest importance, i.e. Hispanic. The Hispanic variable has a large impact on predicting that an observation *does not* have health coverage.

We can use the variable importance generated from the Random Forest to redesign our earlier OLS and Logit models and remove the less important variables.

Prior to that, we should tune our Random Forest model to determine the optimal number of variables used at each split of the data. We can also change the number of trees to make the model easier to compute.

Here is some code to tune the Random Forest for mtry (the number of variables used at each split). The number of trees in the forest has also been reduced to 100. 

```{r Optimal Variable Code for Random Forest, echo=TRUE}
mtry <- tuneRF(sobj$data[-1], as.factor(sobj$data$NOTCOV), ntreeTry = 100, stepFactor = 1.5, improve = 0.05, trace = TRUE, plot = TRUE)
```
```{r mtry}
mtry
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), (1:2)]
print(best.m)
```

The optimal mtry number is 5, which in concert with a smaller number of trees reduced the Out of Box error rate from 13.77% to `r round((best.m[2])*100, digits = 2)`%. This rate tells us that as the model makes the splits and performs prediction tests on internally designated training and test data, it makes errors at a rate of `r round((best.m[2])*100, digits = 2)`%, performing slightly better than when the model is applied to the overall test data. 

### Support Vector Machines
```{r Support Vector Machines}
require(e1071)
# tuned_parameters <- tune.svm(as.factor(NOTCOV) ~ ., data = sobj$data, gamma = 10^(-3:0), cost = 10^(-2:1)) 
# summary(tuned_parameters)
# figure best parameters and input into next
svm.model <- svm(as.factor(NOTCOV) ~ ., data = sobj$data, cost = 10, gamma = 0.1)
svm.pred <- predict(svm.model, s_dat_test)
SVMpredtable <- table(pred = svm.pred, true = dat_test$NOTCOV)
SVMpredtable
SVMproppred <- prop.table(SVMpredtable)
SVMproppred
SVMgoodpred <- sum((SVMproppred[1,1])+(SVMproppred[2,2]))
SVMgoodpred
```
```{r include=FALSE}
svm.model2 <- svm(as.factor(NOTCOV) ~ ., data = sobj$data, cost = 10)
svm.pred2 <- predict(svm.model2, s_dat_test)
SVMpredtable2 <- table(pred = svm.pred2, true = dat_test$NOTCOV)
SVMpredtable2
SVMproppred2 <- prop.table(SVMpredtable2)
SVMproppred2
SVMgoodpred2 <- sum((SVMproppred2[1,1])+(SVMproppred2[2,2]))
```
```{r include=FALSE}
SVMproppred
SVMproppred2
SVMgoodpred
SVMgoodpred2
```
Running the SVM as given produces a correct prediction rate of `r round(SVMgoodpred*100, digits = 2)`%. We can change the SVM model by adjusted the cost and gamma parameters. The cost parameter determines the tolerance the model has to violations of the margins of the separating plane; the higher the cost, the narrower the margin, which produces results with high variance but a lower bias. With higher variance, the model may give [different estimates](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/) for the function of the separating plane when applied to different data sets. If we want to lower the variance to keep estimates produced by the model similar as we change data sets, we need to increase the bias - we have to accept that the model makes more assumptions about the form of the separating function. To keep estimates more consistent across different data sets, we have to accept that the model will lose predictive power.

If we are comfortable with the data set at hand and seek strong predictive power, we can set our cost high, increasing the variance and lowering the bias of the model - similar to a logistic model. If we wish to look at multiple data-sets, to assess correlation and causality, the SVM model has to be adjusted with lower cost, reducing the variance and increasing the bias - similar to an OLS model.

Changing the gamma value in the model adjusts the flexibility of the separating plane. We removed the gamma value from the model and saw that while our True Negative prediction rate increased by 0.74%, our True Positive rate decreased by 0.22%. The trade off of these change in predictions depends on the context of the person asking the questions of the model. For a policy maker, correctly identifying those who lack healthcare coverage is important. Misallocating resources to provide health services to demographics that already have good access to those service is a waste of critical government infrastructure and revenues. For a health insurance company, the tolerance of misclassification may be higher. A person who was predicted to not have coverage can still be pitched with other health insurance products if they already have some sort of coverage.

### Regularized Regression Models (Elastic Net, Ridge, Lasso)
We have run one model for each of the regularized regression models: Elastic Net, Ridge, and Lasso. We ran one for each to be able to see and understand the difference between the three. Lastly, after going through the three models individually, we will compare the prediction results at the end.

As we have made sure to standardize our explanatory variables, the coefficients are all on a common scale and that means the penalty terms we will be applying in these models will equally penalize all predictors (e.g. Age will not be penalized more than female = "1"). 

Being able to remove explanatory variables from our models that have been overloaded with explanatory variables helps make our models more interpretable. We want to remove explanatory variables that are not contributing to our model because as the number of explanatory variables increase, the more likely we will tend to overfit our training data and underfit our testing data. 

#### Elastic Net
Looking at the plot of our cross-validation ridge regression across all the lambda values (*plot(cvmodel1_elasticnet)*), we see a slight improvement in the MSE as our penalty log(lambda) gets larger, which suggests that a regular OLS model likely overfits the training data. But if we continue to constrain the MSE further (i.e increase the penalty), our MSE starts to increase. In this plot we can also see the optimal lambda (the log value of the lambda which best minimizes the error in cross-validation) is -7.772985, as calculated from the function *log(cvmodel1_elasticnet$lambda.min)*. 

We chose to set alpha = 0.5 so that there will be an equal combination of the Lasso penalty and the Ridge penalty.

Next, we want to identify which variables are most important to our model which the elastic net regression can help us with. We run a Variable Important Plot (*vip(cvmodel1_elasticnet, num_features = 50, geom = "col")*) on the cross-validation model of our elastic net regression and see that the advanced degree, bachelor's degree and being married increases our likelihood of having health insurance. Whereas variables such as a high school education being born in the South, your age, being Hispanic or being born in Mexico/Central America/Carribean decrease your likelihood of having health insurance. It is important to note that the variables Hispanic and being born in Mexico/Central American/Caribbean are probably related and have multicollinearity. For this Elastic Net model has some of the ridge penatly incorporated in it since the alpha is set to 0.5 so it can be better fit to handle multicollinearity if we set the alpha closer to 0.

As elastic net performs feature selection (since it includes the Lasso penalty), the next step we would take is to remove the variables who have their coefficients pushed to zero from the model including all the other "born-from," region-based and other variables that show as zero in the Variable Importance Plot below and re-run the elastic net to see how the prediction results improve aka our model becomes more accurate and also easier to interpret.


```{r Elastic Net Regression, eval = FALSE}
# Elastic Net
# install.packages("glmnet")
# install.packages("vip")
require(vip)
require(glmnet)
model1_elasticnet <-  glmnet(as.matrix(sobj$data[,-1]),sobj$data$NOTCOV, alpha = 0.7)

par(mar=c(4.5,4.5,1,4))
plot(model1_elasticnet)
vnat=coef(model1_elasticnet)
vnat=vnat[-1,ncol(vnat)] 
axis(4, at=vnat,line=-.5,label=names(sobj$data[,-1]),las=1,tick=FALSE, cex.axis=0.5) 

plot(model1_elasticnet, xvar = "lambda")
plot(model1_elasticnet, xvar = "dev", label = TRUE)
print(model1_elasticnet)

cvmodel1_elasticnet = cv.glmnet(data.matrix(sobj$data[,-1]),data.matrix(sobj$data$NOTCOV), alpha = 0.7) 
cvmodel1_elasticnet$lambda.min
log(cvmodel1_elasticnet$lambda.min)
coef(cvmodel1_elasticnet, s = "lambda.min")
cvmodel1_elasticnet
plot(cvmodel1_elasticnet)
cvmodel1_elasticnet$lambda.min
vip(cvmodel1_elasticnet, num_features = 50, geom = "col") 

pred1_elasnet <- predict(model1_elasticnet, newx = data.matrix(s_dat_test), s = cvmodel1_elasticnet$lambda.min)
pred_model1_elasnet <- (pred1_elasnet < mean(pred1_elasnet)) 

table(pred = pred_model1_elasnet, true = dat_test$NOTCOV)
ElasticNetpredtable <- table(pred = pred_model1_elasnet, true = dat_test$NOTCOV)
ElasticNetproppred <- prop.table(ElasticNetpredtable)
ElasticNetproppred
ElasticNetgoodpred <- sum((ElasticNetproppred[1,1])+(ElasticNetproppred[2,2]))
ElasticNetgoodpred

model2_elasticnet <-  glmnet(as.matrix(sobj$data[,-1]),sobj$data$NOTCOV, alpha = 0.7) 
model2_elasticnet
```

#### Ridge Regression
Looking at the plot of our cross-validation ridge regression across all the lambda values (*plot(cvmodel1_ridge)*), we see a slight improvement in the MSE as our penalty log(lambda) gets larger, which suggests that a regular OLS model likely overfits the training data. But if we continue to constrain the MSE further (i.e increase the penalty), our MSE starts to increase. In this plot we can also see the optimal lambda (the log value of the lambda which best minimizes the error in cross-validation) is -4.62849, as calculated from the function *log(cvmodel1_ridge$lambda.min)*. 

As ridge regression does not perform feature selection, that is why you see in the Variable Importance Plot below that all variables show some degree of shading of importance. Whereas the Lasso and Elastic Net regressions will zero out/push to zero the variables that are unimportant. As ridge regression performs better when it only has variables that are important, we will re-run a Ridge Regression after removing some of the variables we have identified as less important based on the Lasso and/or Elastic Net Regressions. However, this also means that if we DO have a model where we do not want to drop any of our variables for whatever reason, we can do a ridge regression as it will keep all available features in the final model.


```{r Ridge Regression, eval = FALSE}
# Ridge
# install.packages("glmnet")
# install.packages("vip")
require(vip)
require(glmnet)
model1_ridge <-  glmnet(as.matrix(sobj$data[,-1]),sobj$data$NOTCOV, alpha = 0)

par(mar=c(4.5,4.5,1,4))
plot(model1_ridge)
vnat=coef(model1_ridge)
vnat=vnat[-1,ncol(vnat)] 
axis(4, at=vnat,line=-.5,label=names(sobj$data[,-1]),las=1,tick=FALSE, cex.axis=0.5) 

plot(model1_ridge, xvar = "lambda")
plot(model1_ridge, xvar = "dev", label = TRUE)
print(model1_ridge)

cvmodel1_ridge = cv.glmnet(data.matrix(sobj$data[,-1]),data.matrix(sobj$data$NOTCOV), alpha = 0) 
cvmodel1_ridge$lambda.min
plot(cvmodel1_ridge)
log(cvmodel1_ridge$lambda.min)
coef(cvmodel1_ridge, s = "lambda.min")
vip(cvmodel1_ridge, num_features = 50, geom = "col")

pred1_ridge <- predict(model1_ridge, newx = data.matrix(s_dat_test), s = cvmodel1_ridge$lambda.min)
pred_model1_ridge <- (pred1_ridge < mean(pred1_ridge)) 

table(pred = pred_model1_ridge, true = dat_test$NOTCOV)
Ridgepredtable <- table(pred = pred_model1_ridge, true = dat_test$NOTCOV)
RidgeNetproppred <- prop.table(Ridgepredtable)
RidgeNetproppred
Ridgegoodpred <- sum((RidgeNetproppred[1,1])+(RidgeNetproppred[2,2]))
Ridgegoodpred

model2_ridge <-  glmnet(as.matrix(sobj$data[,-1]),sobj$data$NOTCOV, alpha = 0) 
model2_ridge
```
#### Lasso Regression
Looking at the plot of our cross-validation lasso regression across all the lambda values (*plot(cvmodel1_lasso)*), we see a slight improvement in the MSE as our penalty log(lambda) gets larger, which suggests that a regular OLS model likely overfits the training data. But if we continue to constrain the MSE further (i.e increase the penalty), our MSE starts to increase. In this plot we can also see the optimal lambda (the log value of the lambda which best minimizes the error in cross-validation) is -8.187031, as calculated from the function *log(cvmodel1_lasso$lambda.min)*. 

And as the lasso regression also performs a automated explanatory variable importance selection by using it's ridge penalty to push variables not just approximately to zero, but all the way to zero, we can look at the Variable Important Plot to identify which variables are less important. The Variable Important Plot function is informing us that the "born-in" variables along with the "Region." variables and some other various variables are not important as they have no shading as the coefficients of these variables have been pushed to zero by the lasso regression. Compared to the elastic net regression we ran above, this lasso regression marks the same variables as important as the elastic net, but this lasso regression includes a few more: some college education and female are (both more likely for health insurance) & Race_Other, born Africa, born South America (all three less likely for having health insurance). This makes senses as the Elastic Net includes both the ridge penalty and the lasso penalty: getting effective regularization from the ridge penalty and also the explanatory variable importance selection of the lasso penalty.


```{r Lasso Regression, eval = FALSE}
# Lasso
# install.packages("glmnet")
# install.packages("vip")
require(vip)
require(glmnet)
model1_lasso <-  glmnet(as.matrix(sobj$data[,-1]),sobj$data$NOTCOV, alpha = 1)

par(mar=c(4.5,4.5,1,4))
plot(model1_lasso)
vnat=coef(model1_lasso)
vnat=vnat[-1,ncol(vnat)] 
axis(4, at=vnat,line=-.5,label=names(sobj$data[,-1]),las=1,tick=FALSE, cex.axis=0.5) 

plot(model1_lasso, xvar = "lambda")
plot(model1_lasso, xvar = "dev", label = TRUE)
print(model1_lasso)

cvmodel1_lasso = cv.glmnet(data.matrix(sobj$data[,-1]),data.matrix(sobj$data$NOTCOV),alpha = 1) 
cvmodel1_lasso$lambda.min
plot(cvmodel1_lasso)
log(cvmodel1_lasso$lambda.min)
coef(cvmodel1_lasso, s = "lambda.min")
vip(cvmodel1_lasso, num_features = 50, geom = "col")

pred1_lasso <- predict(model1_lasso, newx = data.matrix(s_dat_test), s = cvmodel1_lasso$lambda.min)
pred_model1_lasso <- (pred1_lasso < mean(pred1_lasso)) 

table(pred = pred_model1_lasso, true = dat_test$NOTCOV)
Lassopredtable <- table(pred = pred_model1_lasso, true = dat_test$NOTCOV)
LassoNetproppred <- prop.table(Lassopredtable)
LassoNetproppred
Lassogoodpred <- sum((LassoNetproppred[1,1])+(LassoNetproppred[2,2]))
Lassogoodpred

model2_lasso <-  glmnet(as.matrix(sobj$data[,-1]),sobj$data$NOTCOV, alpha = 1) 
model2_lasso
```

#### Results Comparison: Regularization Regression
Overall, the correct prediction percentage between the three models are very close, even between the ridge and lasso regressions as that is where you expect to see the largest differences which differ by less than 0.0003%. The elastic net's prediction results are right in between that of the ridge and lasso. Even tweaking the elastic net alpha from 0.5 to 0.7 we only see a slight change that brings the correct prediction percentage closer to the lasso's percentage, which makes sense since the alpha is getting closer to 1 (a lasso regression). 

But all three of these results are significantly less than the ~80%+ we are seeing from Random Forest and Support Vector Machines. 

```{r Comparison, eval = FALSE}
table(pred = pred_model1_elasnet, true = dat_test$NOTCOV)
table(pred = pred_model1_ridge, true = dat_test$NOTCOV)
table(pred = pred_model1_lasso, true = dat_test$NOTCOV)


ElasticNetproppred
RidgeNetproppred
LassoNetproppred


ElasticNetgoodpred
Ridgegoodpred
Lassogoodpred
```
### Other Explanatory Variables
Based on the results from the previous Random Forest, SVM and Lasso models, we noticed that REGION and region_born were consistently unimportant to the models. Age, education, gender and marriage variables were all important, across all models. To test how these models would handle other variables, we removed REGION, region_born and the marriage variables. We created a factor from the person_healthstatus observations and included them in a new subset to test. We noticed that the factor excluded observations that had been coded "Excellent", which will influence how the models make the comparisons between the observations.
```{r HealthStat Varb}
healthstat <- data.frame(model.matrix(~ dat2$person_healthstatus))
dat_for_analysis2 <- data.frame(
  dat2$NOTCOV,
  dat2$AGE_P,
  dat2$female,
  dat2$AfAm,
  dat2$Asian,
  dat2$RaceOther,
  dat2$Hispanic,
  dat2$educ_hs,
  dat2$educ_smcoll,
  dat2$educ_as,
  dat2$educ_bach,
  dat2$educ_adv,
  healthstat[,2:5])
names(dat_for_analysis2) <- c("NOTCOV",
                                 "Age",
                                 "female",
                                 "AfAm",
                                 "Asian",
                                 "RaceOther",
                                 "Hispanic",
                                 "educ_hs",
                                 "educ_smcoll",
                                 "educ_as",
                                 "educ_bach",
                                 "educ_adv",
                                 "Very_Good",
                                 "Good",
                                 "Fair", "Poor")
```
```{r Train Set}
set.seed(654321)
NN2 <- length(dat_for_analysis2$NOTCOV)
NN2
restrict_2 <- as.logical(runif(NN2) < 0.20)
dat_train2 <- subset(dat_for_analysis2, restrict_2)
dat_test2 <- subset(dat_for_analysis2, !restrict_2)
sobj2 <- standardize(NOTCOV ~ Age + female + AfAm + Asian + RaceOther + Hispanic + 
                      educ_hs + educ_smcoll + educ_as + educ_bach + educ_adv + 
                      Very_Good + Good + Fair + Poor, dat_train2, family = binomial)
s_dat_test2 <- predict(sobj2, dat_test2)
```
#### OLS 2
```{r Linear Prob Model 2}
model_OLS2 <- lm(sobj2$formula, data = sobj2$data)
summary(model_OLS2)
pred_vals_OLS2 <- suppressWarnings(predict(model_OLS2, s_dat_test2))
pred_model_OLS2 <- (pred_vals_OLS2 > 0.45)
pred2OLStable <- table(pred = pred_model_OLS2, true = dat_test2$NOTCOV)
pred2OLStable
goodolspred2 <- sum((prop.table(pred2OLStable)[1,1])+(prop.table(pred2OLStable)[2,2]))
```
The estimates for Age, gender and education all have the same signs as they did in the initial OLS model. The health status coefficient estimates suggest that the variables of very good health and poor health correlate towards NOTCOV being zero. Intuition would suggest this is on the right track - those in very good health would have cheaper insurance premiums; those in poor health likely require health insurance to make sure all medical expenses can be covered.

#### Logit 2
```{r Logit 2}
model_logit2 <- glm(sobj2$formula, family = binomial, data = sobj2$data)
summary(model_logit2)
pred_vals2 <- suppressWarnings(predict(model_logit2, s_dat_test2, type = "response"))
pred_model_logit2 <- (pred_vals2 > 0.5)
pred2Logtable <- table(pred = pred_model_logit2, true = dat_test2$NOTCOV)
pred2Logtable
goodlogpred2 <- sum((prop.table(pred2Logtable)[1,1])+(prop.table(pred2Logtable)[2,2]))
# round(goodolspred*100, digits = 2)
```
The results of the Logit model align with those from the OLS model. We see a similar pattern of probability of not having health coverage. Those with more education have a lower probability of not having health insurance coverage. Those with very good health and those in poor health also have a lower probability of not having health insurance. The health status variables in the middle, Good and Fair health, while still low probabilities, tend towards not having health insurance.  

#### Random Forest Model 2
```{r Random Forest 2}
require('randomForest')
set.seed(54321)
model_randFor2 <- randomForest(as.factor(NOTCOV) ~ ., data = sobj2$data, importance=TRUE, proximity=TRUE)
print(model_randFor2)
round(importance(model_randFor2),2)
varImpPlot(model_randFor2)
```
```{r RFPred 2}
pred_model2 <- predict(model_randFor2,  s_dat_test2)
RFpredtable2 <- table(pred = pred_model2, true = dat_test2$NOTCOV)
RFpredtable2
```
```{r Proportions of Correct Prediction 2}
RFproppred2 <- prop.table(RFpredtable2)
# proppred
RFgoodpred2 <- sum((RFproppred2[1,1])+(RFproppred2[2,2]))
# goodpred
RFfalsepos2 <- RFproppred2[2,1]
RFfalseneg2 <- RFproppred2[1,2]
```
#### SVM 2
```{r Support Vector Machines 2}
require(e1071)
# tuned_parameters <- tune.svm(as.factor(NOTCOV) ~ ., data = sobj$data, gamma = 10^(-3:0), cost = 10^(-2:1)) 
# summary(tuned_parameters)
# figure best parameters and input into next
svm2.model <- svm(as.factor(NOTCOV) ~ ., data = sobj2$data, cost = 10, gamma = 0.1)
svm2.pred <- predict(svm2.model, s_dat_test2)
SVM2predtable <- table(pred = svm2.pred, true = dat_test2$NOTCOV)
SVM2predtable
SVM2proppred <- prop.table(SVM2predtable)
SVM2proppred
SVM2goodpred <- sum((SVM2proppred[1,1])+(SVM2proppred[2,2]))
```

#### Elastic Net 2
```{r Elastic Net 2 Regression, eval = FALSE}
# Elastic Net
# install.packages("glmnet")
# install.packages("vip")
require(vip)
require(glmnet)
model1_elasticnet2 <-  glmnet(as.matrix(sobj2$data[,-1]),sobj2$data$NOTCOV, alpha = 0.7)

par(mar=c(4.5,4.5,1,4))
plot(model1_elasticnet2)
vnat=coef(model1_elasticnet2)
vnat=vnat[-1,ncol(vnat)] 
axis(4, at=vnat,line=-.5,label=names(sobj2$data[,-1]),las=1,tick=FALSE, cex.axis=0.5) 

plot(model1_elasticnet2, xvar = "lambda")
plot(model1_elasticnet2, xvar = "dev", label = TRUE)
print(model1_elasticnet2)

cvmodel1_elasticnet2 = cv.glmnet(data.matrix(sobj2$data[,-1]),data.matrix(sobj2$data$NOTCOV), alpha = 0.7) 
cvmodel1_elasticnet2$lambda.min
log(cvmodel1_elasticnet2$lambda.min)
coef(cvmodel1_elasticnet2, s = "lambda.min")
cvmodel1_elasticnet2
plot(cvmodel1_elasticnet2)
cvmodel1_elasticnet2$lambda.min
vip(cvmodel1_elasticnet2, num_features = 50, geom = "col") 

pred1_elasnet2 <- predict(model1_elasticnet2, newx = data.matrix(s_dat_test2), s = cvmodel1_elasticnet2$lambda.min)
pred_model1_elasnet2 <- (pred1_elasnet2 < mean(pred1_elasnet2)) 

table(pred = pred_model1_elasnet2, true = dat_test$NOTCOV)
ElasticNetpredtable2 <- table(pred = pred_model1_elasnet2, true = dat_test$NOTCOV)
ElasticNetproppred2 <- prop.table(ElasticNetpredtable2)
ElasticNetproppred2
ElasticNetgoodpred2 <- sum((ElasticNetproppred2[1,1])+(ElasticNetproppred2[2,2]))
ElasticNetgoodpred2

model2_elasticnet2 <-  glmnet(as.matrix(sobj2$data[,-1]),sobj2$data$NOTCOV, alpha = 0.7) 
model2_elasticnet2
```

#### Ridge Regression 2
```{r Ridge Regression 2, eval = FALSE}
# Ridge
# install.packages("glmnet")
# install.packages("vip")
require(vip)
require(glmnet)
model1_ridge2 <-  glmnet(as.matrix(sobj2$data[,-1]),sobj2$data$NOTCOV, alpha = 0)

par(mar=c(4.5,4.5,1,4))
plot(model1_ridge2)
vnat=coef(model1_ridge2)
vnat=vnat[-1,ncol(vnat)] 
axis(4, at=vnat,line=-.5,label=names(sobj2$data[,-1]),las=1,tick=FALSE, cex.axis=0.5) 

plot(model1_ridge2, xvar = "lambda")
plot(model1_ridge2, xvar = "dev", label = TRUE)
print(model1_ridge2)

cvmodel1_ridge2 = cv.glmnet(data.matrix(sobj2$data[,-1]),data.matrix(sobj2$data$NOTCOV), alpha = 0) 
cvmodel1_ridge2$lambda.min
plot(cvmodel1_ridge2)
log(cvmodel1_ridge2$lambda.min)
coef(cvmodel1_ridge2, s = "lambda.min")
vip(cvmodel1_ridge2, num_features = 50, geom = "col")

pred1_ridge2 <- predict(model1_ridge2, newx = data.matrix(s_dat_test2), s = cvmodel1_ridge2$lambda.min)
pred_model1_ridge2 <- (pred1_ridge2 < mean(pred1_ridge2)) 

table(pred = pred_model1_ridge2, true = dat_test$NOTCOV)
Ridgepredtable2 <- table(pred = pred_model1_ridge2, true = dat_test$NOTCOV)
RidgeNetproppred2 <- prop.table(Ridgepredtable2)
RidgeNetproppred2
Ridgegoodpred2 <- sum((RidgeNetproppred2[1,1])+(RidgeNetproppred2[2,2]))
Ridgegoodpred2

model2_ridge2 <-  glmnet(as.matrix(sobj2$data[,-1]),sobj2$data$NOTCOV, alpha = 0) 
model2_ridge2
```

#### Lasso Regression 2
```{r Lasso Regression 2, eval = FALSE}
# Lasso
# install.packages("glmnet")
# install.packages("vip")
require(vip)
require(glmnet)
model1_lasso2 <-  glmnet(as.matrix(sobj2$data[,-1]),sobj2$data$NOTCOV, alpha = 1)

par(mar=c(4.5,4.5,1,4))
plot(model1_lasso2)
vnat=coef(model1_lasso2)
vnat=vnat[-1,ncol(vnat)] 
axis(4, at=vnat,line=-.5,label=names(sobj2$data[,-1]),las=1,tick=FALSE, cex.axis=0.5) 

plot(model1_lasso2, xvar = "lambda")
plot(model1_lasso2, xvar = "dev", label = TRUE)
print(model1_lasso2)

cvmodel1_lasso2 = cv.glmnet(data.matrix(sobj2$data[,-1]),data.matrix(sobj2$data$NOTCOV),alpha = 1) 
cvmodel1_lasso2$lambda.min
plot(cvmodel1_lasso2)
log(cvmodel1_lasso2$lambda.min)
coef(cvmodel1_lasso2, s = "lambda.min")
vip(cvmodel1_lasso2, num_features = 50, geom = "col")

pred1_lasso2 <- predict(model1_lasso2, newx = data.matrix(s_dat_test2), s = cvmodel1_lasso2$lambda.min)
pred_model1_lasso2 <- (pred1_lasso2 < mean(pred1_lasso2)) 

table(pred = pred_model1_lasso2, true = dat_test$NOTCOV)
Lassopredtable2 <- table(pred = pred_model1_lasso2, true = dat_test$NOTCOV)
LassoNetproppred2 <- prop.table(Lassopredtable2)
LassoNetproppred2
Lassogoodpred2 <- sum((LassoNetproppred2[1,1])+(LassoNetproppred2[2,2]))
Lassogoodpred2

model2_lasso2 <-  glmnet(as.matrix(sobj2$data[,-1]),sobj2$data$NOTCOV, alpha = 1) 
model2_lasso2
```

#### Results Comparison 2: Regularization Regression
So after dropping some of the explanatory variables that were deemed to be less important to the model by the Variable Important Plots and re-running the three regularization regression models, we see that the correct prediction rates actually went down! This is very surprising as we thought  that dropping variables marked as not important would improve our model and therefore improve the prediction accuracy.

It appears that the majority of the shift was from the True Falses, which were at ~53% but are now at ~49%, to the False Positives, which were at ~30% and are now at ~35%. 

```{r Comparison 2, eval = FALSE}
table(pred = pred_model1_elasnet2, true = dat_test$NOTCOV)
table(pred = pred_model1_ridge2, true = dat_test$NOTCOV)
table(pred = pred_model1_lasso2, true = dat_test$NOTCOV)


ElasticNetproppred2
RidgeNetproppred2
LassoNetproppred2


ElasticNetgoodpred2
Ridgegoodpred2
Lassogoodpred2
```


When you summarize, you should be able to explain which models predict best (noting if there is a tradeoff of false positive vs false negative) and if there are certain explanatory variables that are consistently more or less useful. Also try other lists of explanatory variables.


### Bibliography  
[ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NHIS/2014/personsx_layout.pdf](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NHIS/2014/personsx_layout.pdf)

https://www.healthcare.gov/young-adults/children-under-26/

https://bradleyboehmke.github.io/HOML/regularized-regression.html

Brownlee, Jason, (2016). "Support Vector Machines for Machine Learning", https://machinelearningmastery.com/support-vector-machines-for-machine-learning/

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, (2017). "Introduction to Statistical Learning", New York: Springer Science+Business Media, https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf 

Springsteen, Bruce, (1984) "Born In The USA", *Born In The USA*, Columbia, https://www.youtube.com/watch?v=EPhWR4d3FJQ